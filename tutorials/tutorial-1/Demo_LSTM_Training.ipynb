{"cells":[{"cell_type":"markdown","source":["# **Network Training**\n","\n","This tutorial demonstrates how to train the double LSTM-based model for the seqeunce-to-scalar future hysteresis step prediction. The model may serve as a good starting point for the neural network based transient magnetic modeling. The network model will be trained based on 3C90_Training_Tutorial.h5 file and saved as a state dictionary (.sd) file. The training data is a all frequency inclusive, 50-sequences per freqeuncy dataset with each seuqence containing only 1000 randomly selected time steps for training, operating under all availble temepratures, and flux excitations.\n","\n","\n","# **Step 0: Import Packages**\n","\n","In this step we import the important packages that are necessary for the training."],"metadata":{"id":"Tg0WAGktMOVH"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"oE_3CSIzfC1d","outputId":"33f5fb91-0c3a-4d02-e6b6-a962f6f02456","executionInfo":{"status":"ok","timestamp":1747362131880,"user_tz":240,"elapsed":24487,"user":{"displayName":"Princeton Power Electronics","userId":"04815773103473656676"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import random\n","import numpy as np\n","import json\n","import math\n","import csv\n","import time\n","import h5py"]},{"cell_type":"markdown","source":["# **Step 1: Define Network Structure**\n","The structure of the duel LSTM-based encoder-projector-decoder neural network are defined here. Refer to the PyTorch document for more details."],"metadata":{"id":"ENIjgL-si0I2"}},{"cell_type":"code","source":["# Define model structures and functions\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        self.lstm_B = nn.LSTM(1, 12, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        self.lstm_H = nn.LSTM(1, 12, num_layers=1, batch_first=True, bidirectional=False)\n","\n","        self.projector = nn.Sequential(\n","            nn.Linear(12 *2 + 2 , 12 *2 + 2),\n","            nn.ReLU(),\n","            nn.Linear(12 *2 + 2, 8),\n","            nn.ReLU(),\n","            nn.Linear(8 , 4),\n","            nn.ReLU(),\n","            nn.Linear(4, 1)\n","        )\n","\n","    def forward(self, seq_B: Tensor, seq_H: Tensor, scal: Tensor, T: Tensor, device) -> Tensor:\n","\n","        seq_B = seq_B.float()\n","        seq_H = seq_H.float()\n","        scal = scal.float()\n","        T = T.float()\n","        x_B, _ = self.lstm_B(seq_B)\n","        x_B = x_B[:, -1, :]\n","        x_H, _ = self.lstm_H(seq_H)\n","        x_H = x_H[:, -1, :]\n","\n","        # print(x_B.size())\n","        # print(x_H.size())\n","        # print(scal.size())\n","        output = self.projector(torch.cat((scal, T, x_B, x_H), dim=1))\n","        output = output.to(device)\n","\n","        return output"],"metadata":{"id":"dYMQPMknLUPN","executionInfo":{"status":"ok","timestamp":1747362134794,"user_tz":240,"elapsed":19,"user":{"displayName":"Princeton Power Electronics","userId":"04815773103473656676"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# **Step 2: Load the Training Dataset**\n","\n","Dataset needs to be processed before trianing. This includes the normalization of the data. In this part, we load and pre-process the dataset for the network training and testing. In this demo, a small dataset measured with 3C90 ferrite material containing all frequency, temperature, flux excitations is used. The full dataset can be downloaded from the MagNet Challenge GitHub repository."],"metadata":{"id":"2CXI2TIOpNJl"}},{"cell_type":"code","source":["# Load the dataset\n","def get_dataset(data_length=80):\n","\n","    # Change the directory to where the training data is located. The file used is h5py file. For more inofmration, please visit https://docs.h5py.org/en/latest/build.html\n","    with h5py.File('/content/drive/MyDrive/Colab Notebooks/Minjie Chen/MagNetX/3C90_Training.h5', 'r') as file:\n","        print(\"keys:\", list(file.keys()))\n","\n","        B_list = []\n","        H_list = []\n","        B_scal_list = []\n","        H_out_list = []\n","        T_scal_list = []\n","\n","        for i in range(1, 8):  # i from 1 to 7, for reading all frequency datasets\n","            B_list.append(file[f'B_seq_f_{i}'][:])\n","            H_list.append(file[f'H_seq_f_{i}'][:])\n","            B_scal_list.append(file[f'B_scal_{i}'][:])\n","            H_out_list.append(file[f'H_scal_{i}'][:])\n","            T_scal_list.append(file[f'T_{i}'][:])\n","\n","\n","        # Now concatenate all of them after the loop\n","        B = np.concatenate(B_list, axis=0)\n","        H = np.concatenate(H_list, axis=0)\n","        B_scal = np.concatenate(B_scal_list, axis=0)\n","        H_out = np.concatenate(H_out_list, axis=0)\n","        T_scal = np.concatenate(T_scal_list, axis=0)\n","\n","    print(\"Data Loading Initiated\")\n","\n","\n","    B = np.array(B)\n","    H = np.array(H)\n","    B_scal = np.array(B_scal)\n","    H_out = np.array(H_out)\n","    T_scal = np.array(T_scal)\n","\n","    B_scal = B_scal.reshape(-1,1)\n","    B_scal = torch.from_numpy(B_scal)\n","    T_scal = T_scal.reshape(-1,1)\n","    T_scal = torch.from_numpy(T_scal)\n","\n","    in_B = torch.from_numpy(B).float().view(-1,data_length,1)\n","    in_H = torch.from_numpy(H).float().view(-1,data_length,1)\n","    out = torch.from_numpy(H_out).float().view(-1,1)\n","\n","\n","    # Save the normalized parameters to JSON file\n","    with open('/content/drive/MyDrive/Colab Notebooks/Minjie Chen/MagNetX/Normalization_Params.json', 'w') as f:\n","        json.dump({'mean_B': torch.mean(in_B).tolist(),\n","                    'std_B': torch.std(in_B).tolist(),\n","                    'mean_H': torch.mean(in_H).tolist(),\n","                    'std_H': torch.std(in_H).tolist(),\n","                    'mean_out': torch.mean(out).tolist(),\n","                    'std_out': torch.std(out).tolist(),\n","                    'mean_Scal': torch.mean(B_scal).tolist(),\n","                    'std_Scal': torch.std(B_scal).tolist(),\n","                    'mean_T': torch.mean(T_scal).tolist(),\n","                    'std_T': torch.std(T_scal).tolist()},f)\n","\n","    B_scal = (B_scal-torch.mean(B_scal))/torch.std(B_scal)\n","    T_scal = (T_scal-torch.mean(T_scal))/torch.std(T_scal)\n","    in_B = (in_B-torch.mean(in_B))/torch.std(in_B)\n","    in_H = (in_H-torch.mean(in_H))/torch.std(in_H)\n","    out = (out-torch.mean(out))/torch.std(out)\n","\n","\n","\n","    print(in_B.size())\n","    print(in_H.size())\n","    print(B_scal.size())\n","    print(out.size())\n","\n","\n","\n","    return torch.utils.data.TensorDataset(in_B, in_H , B_scal, T_scal, out)"],"metadata":{"id":"4O__WD8JLUjt","executionInfo":{"status":"ok","timestamp":1747362138606,"user_tz":240,"elapsed":50,"user":{"displayName":"Princeton Power Electronics","userId":"04815773103473656676"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# **Step 3: Training the Model**\n","\n","The loaded dataset is randomly split into training set, validation set, and test set. The output of the training is saved into the state dictionary file (.sd) containing all the trained parameter values. In this exmaple however, the test set data is from a seperate file."],"metadata":{"id":"DJcX3ZPesurO"}},{"cell_type":"code","source":["# Defind parameters\n","# Define the memory length your data is saved with\n","data_length = 80\n","\n","# Count the number of parameters\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","\n","# Config the model training\n","\n","def main():\n","\n","    # Reproducibility\n","    random.seed(1)\n","    np.random.seed(1)\n","    torch.manual_seed(1)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    # Hyperparameters\n","    NUM_EPOCH = 200\n","    BATCH_SIZE = 2048\n","    DECAY_EPOCH = 50\n","    DECAY_RATIO = 0.6\n","    LR_INI = 0.001\n","\n","    times = []\n","    results = []\n","\n","    # Select GPU as default device\n","    device = torch.device(\"cuda\")\n","\n","    # Load dataset\n","    dataset = get_dataset()\n","\n","    # Split the dataset\n","    train_size = int(0.8 * len(dataset))\n","    valid_size = int(0.1 * len(dataset))\n","    test_size = len(dataset)- train_size- valid_size\n","    train_dataset, valid_dataset, test_dataset= torch.utils.data.random_split(dataset, [train_size, valid_size, test_size])\n","    kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n","    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, **kwargs)\n","    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","    test_loader =  torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n","    trainData = list(train_loader)\n","    validData = list(valid_loader)\n","    testData = list(test_loader) # Not used in training, have a seperate dataset for the test file\n","\n","    # Setup network\n","    net = Net().to(device)\n","\n","    # Log the number of parameters\n","    print(\"Number of parameters: \", count_parameters(net))\n","    print(\"Number of parameters: \", count_parameters(net.lstm_B))\n","    print(\"Number of parameters: \", count_parameters(net.projector))\n","\n","    # Setup optimizer\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(net.parameters(), lr=LR_INI)\n","\n","    # Record initial time\n","    start_time = time.time()\n","\n","    # Train the network\n","    for epoch_i in range(NUM_EPOCH):\n","\n","        start_epoch = time.time()\n","\n","        # Train for one epoch\n","        epoch_train_loss = 0\n","        net.train()\n","        optimizer.param_groups[0]['lr'] = LR_INI* (DECAY_RATIO ** (0+ epoch_i // DECAY_EPOCH))\n","\n","        for in_B, in_H, B_scal, T_scal, out in trainData:\n","            optimizer.zero_grad()\n","            output = net(seq_B=in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device) , device=device)\n","            loss = criterion(output, out.to(device))\n","            loss.backward()\n","\n","            optimizer.step()\n","            epoch_train_loss += loss.item()\n","\n","        # Compute Validation Loss\n","        with torch.no_grad():\n","            epoch_valid_loss = 0\n","            for in_B, in_H, B_scal, T_scal, out in validData:\n","                output_valid = net(seq_B=in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device), device=device)\n","                loss = criterion(output_valid, out.to(device))\n","\n","\n","                epoch_valid_loss += loss.item()\n","\n","\n","        end_epoch = time.time()\n","        times.append(end_epoch-start_epoch)\n","\n","\n","        # Save the model parameters\n","        if (epoch_i+1)%100 == 0:\n","            torch.save(net.state_dict(), \"/content/drive/MyDrive/Colab Notebooks/Minjie Chen/MagNetX/Model_LSTM_New\"+\".sd\")\n","            print(\"Model is saved!\")\n","\n","\n","    elapsed = time.time() - start_time\n","    print(f\"Total Time Elapsed: {elapsed}\")\n","    print(f\"Average time per Epoch: {sum(times)/NUM_EPOCH}\")\n","\n","\n","    # Test Evaluation\n","    net.eval()\n","    y_meas = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for in_B, in_H, B_scal, T_scal, out in testData:\n","            y_pred.append(net(seq_B=in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device) , device=device))\n","            y_meas.append(out.to(device))\n","\n","    y_meas = torch.cat(y_meas, dim=0)\n","    y_pred = torch.cat(y_pred, dim=0)\n","    print(f\"Test Loss: {F.mse_loss(y_meas, y_pred).item() / len(test_dataset) * 1e5:.5f}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"wmMVUUOCusfX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8dd3acec-365a-44d1-fd08-f261f72850ff","executionInfo":{"status":"ok","timestamp":1747362166203,"user_tz":240,"elapsed":21904,"user":{"displayName":"Princeton Power Electronics","userId":"04815773103473656676"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["keys: ['B_scal_1', 'B_scal_2', 'B_scal_3', 'B_scal_4', 'B_scal_5', 'B_scal_6', 'B_scal_7', 'B_seq_f_1', 'B_seq_f_2', 'B_seq_f_3', 'B_seq_f_4', 'B_seq_f_5', 'B_seq_f_6', 'B_seq_f_7', 'H_scal_1', 'H_scal_2', 'H_scal_3', 'H_scal_4', 'H_scal_5', 'H_scal_6', 'H_scal_7', 'H_seq_f_1', 'H_seq_f_2', 'H_seq_f_3', 'H_seq_f_4', 'H_seq_f_5', 'H_seq_f_6', 'H_seq_f_7', 'T_1', 'T_2', 'T_3', 'T_4', 'T_5', 'T_6', 'T_7']\n","Data Loading Initiated\n","torch.Size([17500, 80, 1])\n","torch.Size([17500, 80, 1])\n","torch.Size([17500, 1])\n","torch.Size([17500, 1])\n","Number of parameters:  2399\n","Number of parameters:  720\n","Number of parameters:  959\n","Model is saved!\n","Model is saved!\n","Total Time Elapsed: 12.322002410888672\n","Average time per Epoch: 0.06153782725334168\n","Test Loss: 9.51526\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}