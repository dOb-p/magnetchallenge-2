{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tg0WAGktMOVH"
   },
   "source": [
    "# **1: Network Testing**\n",
    "\n",
    "This tutorial demonstrates how to evaluate the double LSTM-based model for the sequence-to-scalar future hysteresis step prediction. The testset seqeunces include data that has the same sampling time steps as the training dataset. However the code is able to decipher the sequence length (provided that it has a sampling frequency), and upsample or downsample as needed to fit the model criteria. If the sequence is longer than the designed memory, then only the most recent time steps (amount to the total training memory time) are taken. If the data length is shorter, then the extra empty memory will repeat the constant of the most distant memory data point. \n",
    "\n",
    "\n",
    "# **Step 0: Import Packages**\n",
    "\n",
    "In this step we import the important packages that are necessary for the testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 34688,
     "status": "ok",
     "timestamp": 1741043766952,
     "user": {
      "displayName": "Shukai Wang",
      "userId": "00237405294920314802"
     },
     "user_tz": 300
    },
    "id": "oE_3CSIzfC1d",
    "outputId": "3cbc82eb-f205-4222-8071-ae37b22283f3"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import math\n",
    "import csv\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ENIjgL-si0I2"
   },
   "source": [
    "# **Step 1: Define Network Structure**\n",
    "The structure of the duel LSTM-based encoder-projector neural network are defined here. The network structure does not change from the training structure. Refer to the PyTorch document for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dYMQPMknLUPN"
   },
   "outputs": [],
   "source": [
    "# Define model structures and functions\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.lstm_B = nn.LSTM(1, 6, num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.lstm_H = nn.LSTM(1, 6, num_layers=1, batch_first=True, bidirectional=False)\n",
    "\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(6 *2 + 2 , 6 *2 + 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(6 *2 + 2, 8),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(8 , 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, seq_B: Tensor, seq_H: Tensor, scal: Tensor, T: Tensor, device) -> Tensor:\n",
    "\n",
    "        seq_B = seq_B.float()\n",
    "        seq_H = seq_H.float()\n",
    "        scal = scal.float()\n",
    "        T = T.float()\n",
    "\n",
    "        x_B, _ = self.lstm_B(seq_B)\n",
    "        x_B = x_B[:, -1, :]\n",
    "\n",
    "        x_H, _ = self.lstm_H(seq_H)\n",
    "        x_H = x_H[:, -1, :]\n",
    "\n",
    "\n",
    "        output = self.projector(torch.cat((scal, T, x_B, x_H), dim=1))\n",
    "        output = output.to(device)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CXI2TIOpNJl"
   },
   "source": [
    "# **Step 2: Load the Testing Dataset**\n",
    "\n",
    "Dataset needs to be processed before testing. The provided dataset includes multiple segment split ratios. Three ratios, 10:90%, 50:50%, and 90:10% data ratios are provided. Output H sequence is post processed again to provide autoregressive function. User input should provide the information of sampling time, which is the same as the model in this Challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parameters\n",
    "def get_dataset(sample_freq_model, B, H, H_true, T, data_length, path_root):\n",
    "\n",
    "    # Load from JSON\n",
    "    Norm_file_name = 'Normalization_Params.json'\n",
    "    with open(path_root+Norm_file_name, 'r') as f:\n",
    "        Param = json.load(f)\n",
    "        \n",
    "    # Initialization of the normalization Data for each material\n",
    "    print(\"Normalization Initiated\")\n",
    "    mean_B = np.array(Param['mean_B'])\n",
    "    std_B = np.array(Param['std_B'])\n",
    "    mean_H = np.array(Param['mean_H'])\n",
    "    std_H = np.array(Param['std_H'])\n",
    "    mean_out = np.array(Param['mean_out'])\n",
    "    std_out = np.array(Param['std_out'])\n",
    "    mean_Scal = np.array(Param['mean_Scal'])\n",
    "    std_Scal = np.array(Param['std_Scal'])    \n",
    "    mean_T = np.array(Param['mean_T'])    \n",
    "    std_T = np.array(Param['std_T'])   \n",
    "\n",
    "    H_len = H[0].size\n",
    "    sample_freq_test = sample_freq_model  # Sampling frequency for testing sequence set the same as model sampling frequency.\n",
    "    sample_time_test = 1/sample_freq_test\n",
    "    total_time_H = (H_len - 1) * sample_time_test\n",
    "\n",
    "    data_length_model = data_length\n",
    "    sample_time_model = 1/sample_freq_model\n",
    "\n",
    "    data_length_test = H_len              # memory data length of testing dataset (as B is a full sequence including past and future points, and H has only past memory sequence)\n",
    "    \n",
    "    # Time step difference\n",
    "    if total_time_H != sample_time_model * (data_length_model-1):\n",
    "        if data_length_model > data_length_test: # when testing data length is shorter\n",
    "            # Fill the empty elements with the same value as the first value of the original sequence\n",
    "            B_ftr = B[:,data_length_test:]\n",
    "            H_ftr = H_true[:,data_length_test:]\n",
    "            B = np.concatenate([np.tile(B[:,0].reshape(-1,1),(1,data_length_model - data_length_test)),B[:, : data_length_test]],axis = 1) \n",
    "            H = np.concatenate([np.tile(H[:,0].reshape(-1,1),(1,data_length_model - data_length_test)),H[:, : data_length_test]],axis = 1)\n",
    "        else: # when testing data length is longer\n",
    "            # Use only the recent data_length values\n",
    "            B_ftr = B[:,data_length_test:]\n",
    "            H_ftr = H_true[:,data_length_test:]\n",
    "            B = B[:,data_length_test - data_length_model:data_length_test]\n",
    "            H = H[:,data_length_test - data_length_model:data_length_test]\n",
    "\n",
    "\n",
    "    in_B = B\n",
    "    in_H = H\n",
    "    in_B_next = B\n",
    "    B_scal = B_ftr[:, [0]]\n",
    "    H_out = H_ftr[:, [0]]\n",
    "    T_scal = T\n",
    "\n",
    "\n",
    "    for i in range(len(B_ftr[0])-1):\n",
    "        in_B_next = np.roll(in_B_next, -1, axis = 1)\n",
    "        in_B_next[:, -1] = B_ftr[:, i]\n",
    "\n",
    "        B_scal = np.vstack([B_scal, B_ftr[:, [i+1]]])\n",
    "        H_out = np.vstack([H_out, H_ftr[:, [i+1]]])\n",
    "        T_scal = np.vstack([T_scal, T])\n",
    "        in_B = np.vstack([in_B, in_B_next])\n",
    "        in_H = np.vstack([in_H, np.zeros_like(H)])\n",
    "\n",
    "\n",
    "\n",
    "    B_scal = (B_scal-mean_Scal)/std_Scal\n",
    "    T_scal = (T_scal-mean_T)/std_T\n",
    "    in_B = (in_B-mean_B)/std_B\n",
    "    in_H = (in_H-mean_H)/std_H\n",
    "    H_out = (H_out-mean_out)/std_out\n",
    "\n",
    "    normH = [mean_out, std_out]\n",
    "\n",
    "    in_B = torch.from_numpy(in_B).float().view(-1,data_length_model, 1)\n",
    "    in_H = torch.from_numpy(in_H).float().view(-1,data_length_model, 1)\n",
    "    out = torch.from_numpy(H_out).float().view(-1,1)\n",
    "    B_scal = torch.from_numpy(B_scal).float().view(-1,1)\n",
    "    T_scal = torch.from_numpy(T_scal).float().view(-1,1)\n",
    "\n",
    "    return torch.utils.data.TensorDataset(in_B, in_H, B_scal, T_scal, out), normH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJcX3ZPesurO"
   },
   "source": [
    "# **Step 3: Testing the Model**\n",
    "\n",
    "The loaded dataset is directly used as the test set. The model state dictionary file (.sd) containing all the trained parameter values is loaded and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defind parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Config the model testing\n",
    "def Test(sample_freq_model, B, H, H_true, T, data_length, path_root, state_dict):\n",
    "\n",
    "    # Load dataset in iterations\n",
    "    idx_nan = np.argmax(np.isnan(H), axis = 1) #  Find the column idx for each row where NaN occurs. idx_nan is the future B input, memory should precede this point\n",
    "    has_nan = np.any(np.isnan(H), axis = 1) # Find all column idx where NaN occurs\n",
    "    idx_nan[~has_nan] = -1 \n",
    "    lgt_change = np.where(np.diff(idx_nan))[0] + 1  # Find the row idx where the NaN changes occur\n",
    "    start = np.insert(lgt_change, 0 , 0) # Start idx for each ratios of partial data\n",
    "    stop = np.append(lgt_change, len(idx_nan))  \n",
    "\n",
    "# Perform the evulation iteratively by each data segment split ratio\n",
    "    for i, (start, stop) in enumerate(zip(start, stop)):\n",
    "        idx_st = idx_nan[start]  # Row starting idx \n",
    "        B_seq = B[start:stop, :]\n",
    "        H_mem = H[start:stop, 0 : idx_st]\n",
    "        T_scal = T[start:stop, :]\n",
    "        H_meas = H_true[start:stop, :]\n",
    "\n",
    "        # Hyperparameters\n",
    "        BATCH_SIZE = len(idx_nan[start : stop])\n",
    "\n",
    "        # Load dataset\n",
    "        dataset, normH = get_dataset(sample_freq_model, B_seq, H_mem, H_meas, T_scal, data_length, path_root)\n",
    "    \n",
    "        print(f\"Data Iteration {i+1}\")\n",
    "    \n",
    "       # Reproducibility\n",
    "        random.seed(1)\n",
    "        np.random.seed(1)\n",
    "        torch.manual_seed(1)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "        # Select GPU as default device\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "        # Split the dataset\n",
    "        kwargs = {'num_workers': 0, 'pin_memory': True, 'pin_memory_device': \"cuda\"}\n",
    "        test_loader =  torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, **kwargs)\n",
    "        testData = list(test_loader)\n",
    "\n",
    "\n",
    "        # Setup network\n",
    "        net = Net().to(device)\n",
    "\n",
    "        # Log the number of parameters\n",
    "        print(\"Number of parameters: \", count_parameters(net))\n",
    "\n",
    "        # Load trained parameters\n",
    "        net.load_state_dict(state_dict, strict=True)\n",
    "        net.eval()\n",
    "        print(\"Model is loaded!\")\n",
    "\n",
    "\n",
    "        #Test\n",
    "        net.eval()\n",
    "        out_pred = []\n",
    "        out_pred = torch.tensor(out_pred)\n",
    "        out_meas = []\n",
    "        out_meas = torch.tensor(out_meas)\n",
    "        outputs = torch.empty((0,1))\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "  \n",
    "    \n",
    "        previous_in_H= None\n",
    "        with torch.no_grad():\n",
    "            for in_B, in_H, B_scal, T_scal, out in testData:  # Batch level\n",
    "\n",
    "                if  previous_in_H is None:\n",
    "                    outputs = net(seq_B = in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device), device = device)\n",
    "                    out_pred = out_pred.to(outputs.device)\n",
    "                    out_pred = torch.cat((out_pred, outputs), dim = 1)\n",
    "                    out_meas = out_meas.to(out.device)\n",
    "                    out_meas = torch.cat((out_meas, out), dim = 1)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    # Update in_H autoregressively with previous prediction\n",
    "                    in_H = np.roll(previous_in_H.cpu(),-1,axis = 1)\n",
    "                    in_H[:, -1] = out_pred[:, -1].cpu().numpy().reshape(-1, 1)  # Set the new value at the end\n",
    "                    in_H = torch.from_numpy(in_H).float()\n",
    "\n",
    "                    outputs = net(seq_B = in_B.to(device), seq_H = in_H.to(device), scal = B_scal.to(device), T = T_scal.to(device), device = device)\n",
    "                    out_pred = out_pred.to(outputs.device)\n",
    "                    out_pred = torch.cat((out_pred, outputs), dim = 1)\n",
    "                    out_meas = out_meas.to(out.device)\n",
    "                    out_meas = torch.cat((out_meas, out), dim = 1)\n",
    "\n",
    "\n",
    "                previous_in_H = in_H\n",
    "\n",
    "            # Obtain the tested data\n",
    "            y_meas = out_meas.cpu().numpy()\n",
    "            y_meas = y_meas *normH[1]+normH[0]\n",
    "            y_pred = out_pred.cpu().numpy()\n",
    "            y_pred = y_pred *normH[1]+normH[0]\n",
    "            \n",
    "\n",
    "            # Save the test data into csv files\n",
    "            with open(path_root + \"Testing/pred.csv\", \"a\") as f:\n",
    "                np.savetxt(f, y_pred, delimiter=',')\n",
    "                f.close()\n",
    "            with open(path_root + \"Testing/meas.csv\", \"a\") as f:\n",
    "                np.savetxt(f, y_meas, delimiter=',')\n",
    "                f.close()\n",
    "\n",
    "            print(\"Testing finished! Results are saved!\")\n",
    "\n",
    "\n",
    "        print(\"Test iteration completed!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Labadmin\\AppData\\Local\\Temp\\ipykernel_10264\\1066713361.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path_root + 'Model_LSTM_3C90.sd')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Initiated (CSV)\n",
      "Normalization Initiated\n",
      "Data Iteration 1\n",
      "Number of parameters:  771\n",
      "Model is loaded!\n",
      "Testing finished! Results are saved!\n",
      "Test iteration completed!\n",
      "Normalization Initiated\n",
      "Data Iteration 2\n",
      "Number of parameters:  771\n",
      "Model is loaded!\n",
      "Testing finished! Results are saved!\n",
      "Test iteration completed!\n",
      "Normalization Initiated\n",
      "Data Iteration 3\n",
      "Number of parameters:  771\n",
      "Model is loaded!\n",
      "Testing finished! Results are saved!\n",
      "Test iteration completed!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    Material = '3C90'\n",
    "    path_root = 'C:/Users/Labadmin/' + Material + '/'\n",
    "\n",
    "    # File name\n",
    "    file_B = f\"{Material}_Testing_True_B_seq.csv\"\n",
    "    file_H = f\"{Material}_Testing_Padded_H_seq.csv\"\n",
    "    file_T = f\"{Material}_Testing_True_T.csv\"\n",
    "    file_H_true = f\"{Material}_Testing_True_H_seq.csv\"\n",
    "\n",
    "    state_dict = torch.load(path_root + 'Model_LSTM_3C90.sd')\n",
    "\n",
    "    B = pd.read_csv(path_root + file_B, header=None).to_numpy()\n",
    "    H = pd.read_csv(path_root + file_H, header=None).to_numpy()\n",
    "    T_scal = pd.read_csv(path_root + file_T, header=None).to_numpy().reshape(-1, 1)\n",
    "    H_true = pd.read_csv(path_root + file_H_true, header=None).to_numpy()\n",
    "    \n",
    "    print(\"Data Loading Initiated (CSV)\")\n",
    "\n",
    "    data_length = 80 # Memory data length\n",
    "    sample_freq_model = 16e6 # Sampling frequency from model training\n",
    "    # Run the test function\n",
    "    Test(sample_freq_model, B, H, H_true, T_scal, data_length, path_root, state_dict)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMUtNOJpB+v1CYXAsgvlAWC",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
